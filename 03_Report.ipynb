{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which techniques you have used while cleaning the data if you have cleaned it ? \n",
    "\n",
    "**NLTK**\n",
    "\n",
    " - The Natural Language Toolkit, a python library I am using for stop words and word tokenize.\n",
    " \n",
    "**Regular Expression**\n",
    "\n",
    " - By looking at the data there a lot of common punctuation we don't need like \"[],'\"-\" and others so I customize a pattern that remove these punctuation.\n",
    " \n",
    "**Remove duplicate rows**\n",
    "\n",
    "**Remove rows with Nan values**\n",
    " \n",
    " - Nan values of some columns cause error when we made our prediction with most job Function like:\n",
    "\n",
    "    ![alt text](images/null_image.png \"Nan Error\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does your model work ? \n",
    "\n",
    "**recommendation model**\n",
    "\n",
    " - Our model work with some of sequences functions that work in the following manner:\n",
    "     - We start in recommendation model to clean the file we pass as a parameter along with user job_title.\n",
    "         - At file_cleaning we clean some of punctuation like '][,\"\\'/-_'] then we remove any stop words like and, or which has no meaning in our process.\n",
    "     - After Cleaning file we start to get most relevant job Functions using most_related_job Function function:\n",
    "         - This function help us get most Job functions by weighted similarity and compute jaccard_similarity_score functions.\n",
    "             - The functions above return with most weighted job title that user inputs.\n",
    " \n",
    " \n",
    "**Returned Function**\n",
    "\n",
    " - After We have get most relvent job function we use RESTful API along with Flask to display our result like:\n",
    " ![alt text](images/video_job_2.png \"JobFunction\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does your model work ? \n",
    "\n",
    "I have used this Approach because our data is small with multi duplicate data which make it smaller to run Machine Learning algorithms that is has one feature \"univariate data\" and that actually make the model learn so week , but with **Jaccard Similarity function** it help a lot for finding most relevant job title for user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can you extend the model to have better performance ? \n",
    "\n",
    "**Data**\n",
    "   - I think one of the most thing we need to make our model work better and run Machine Learning algorithms we need to extend our features to be **Multivariate Features.**\n",
    "   - Also more training examples help a lot to make a model learn more about data.\n",
    "   \n",
    "**Models**\n",
    "   - With More data we can run different Machine Learning models and splitting out data to training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  How do you evaluate your model? \n",
    "\n",
    " - I didn't use any Machine Learning Algorithm because of our data is very small so I don't using matrix evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  What are the limitations of your methodology or where does your approach fail ?\n",
    "I think limitations of my model is to extend the data then we can mapping our features to the output \"job Function\" which will help us to learn from these data with **Multivariate Features and more training examples.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create this script as a RESTful API service\n",
    "\n",
    "I am using A RestFull along with Flask to run my model on Spyder instead of the notebook, also I have run test cases with jupyter which attached with the folder data.\n",
    "\n",
    "**The follwing code represent API with Flask**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# app = Flask(__name__)\n",
    "# @app.route('/job_function/', methods=['GET'])\n",
    "\n",
    "# def job_function():\n",
    "#     Job_title = request.args['Job_title']\n",
    "#     Job_title = clean_text(Job_title).lower()\n",
    "#     most_jobfunction = recommendation_model(df_file, Job_title)\n",
    "#     job_functions= set()\n",
    "#     for val in most_jobfunction:\n",
    "#         words = remove_stop_words(val)\n",
    "#         for word in words:\n",
    "#             job_functions.add(word.upper())\n",
    "#     job_functions = list(job_functions)\n",
    "#     return jsonify({\"Most Related function for job title of \"+str(Job_title): job_functions})\n",
    "\n",
    "\n",
    "\n",
    "# @app.route('/', methods=['GET'])\n",
    "\n",
    "# def job_title():\n",
    "#     Job_title = request.args['Job_title']\n",
    "#     return redirect(\"http://0.0.0.0:8066/job_function/?Job_title=\"+Job_title, code=302)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     from waitress import serve\n",
    "#     serve(app, host=\"0.0.0.0\", port=8066)\n",
    "# '''\n",
    "# print(\"================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All code in Spyder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # 1 - Packages\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import nltk\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "# nltk.download('stopwords')\n",
    "# from flask import Flask, jsonify, request, redirect\n",
    "# import os\n",
    "# ## 2 - Read Data\n",
    "\n",
    "# full_path = os.path.realpath(__file__)\n",
    "# path, filename = os.path.split(full_path)\n",
    "# df_file = pd.read_csv(path+'/jobs_data.csv')\n",
    "\n",
    "# print(\"First 5 entery of our Data\\n\", df_file.head())\n",
    "# df_file = df_file.drop_duplicates(subset='title')\n",
    "\n",
    "\n",
    "# '''\n",
    "# # PreProcessing Data\n",
    "\n",
    "# ## before we start to take a step  we need to handle our data\n",
    "\n",
    "# **some of these handles are:**\n",
    "#     - braces\n",
    "#     - punctuation\n",
    "#     - quotes\n",
    "#     - Tokenization â€” convert sentences to words\n",
    "#     - Stemming  back word to its root like studying - study\n",
    "#     -  under_score ,stop words and other depends on user input because our data cleaned some of these \n",
    "\n",
    "# '''\n",
    "\n",
    "# def clean_text(words):\n",
    "#     '''\n",
    "#     a fcuntion return cleaned text from punctuation, braces, quotes and others\n",
    "#     argument:\n",
    "#         string of words\n",
    "#     return:\n",
    "#         cleaned string\n",
    "#     '''\n",
    "#     words = list(words)\n",
    "#     reqular_expression = '][,\"\\'/-_'\n",
    "#     cleaned_words = \"\"\n",
    "#     for word in words:\n",
    "#         one_cleaned_word = \"\"\n",
    "#         word = word.replace('/', ' ')\n",
    "#         word = word.replace(']', ' ')\n",
    "#         word = word.replace('[', ' ')\n",
    "#         for c in word:\n",
    "#             if not c.isdigit() and c not in reqular_expression:\n",
    "#                 one_cleaned_word +=c\n",
    "#         cleaned_words +=one_cleaned_word\n",
    "#     return cleaned_words\n",
    "\n",
    "# def remove_stop_words(words):\n",
    "#     '''\n",
    "#     a function return list of words without stop words\n",
    "#     stop words like the,and, over and others english stopwords\n",
    "#     argument:\n",
    "#         string of words\n",
    "#     return:\n",
    "#         list of words\n",
    "#     '''\n",
    "#     words = word_tokenize(words)\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     words = [w for w in words if not w in stop_words]\n",
    "#     return words\n",
    "\n",
    "\n",
    "# def file_cleaning(file):\n",
    "#     '''\n",
    "#     a function that return over all cleaned file\n",
    "#     argument:\n",
    "#         csv file\n",
    "#     return:\n",
    "#         cleaned file\n",
    "#     '''\n",
    "#     for key, value in file.iterrows():\n",
    "#         value['title'] = clean_text(value['title'])\n",
    "#         value['jobFunction'] = clean_text(value['jobFunction'])\n",
    "#         cleaned_title = remove_stop_words(value['title'])\n",
    "#         cleaned_function = remove_stop_words(value['jobFunction'])\n",
    "#         value['title'] = ' '.join(map(str,cleaned_title)).lower()\n",
    "#         value['jobFunction'] = ' '.join(map(str,cleaned_function)).lower()\n",
    "#     return file\n",
    "\n",
    "\n",
    "# def compute_jaccard_similarity_score(x, y):\n",
    "#     \"\"\"\n",
    "#         a function to return similarities between two string\n",
    "#     argument:\n",
    "#         x as  string which user_job_title\n",
    "#         y for each of job title in our file\n",
    "#     \"\"\"\n",
    "#     intersection_cardinality = len(set(x).intersection(set(y)))\n",
    "#     union_cardinality = len(set(x).union(set(y)))\n",
    "#     return intersection_cardinality / float(union_cardinality)\n",
    "\n",
    "\n",
    "# def weighted_similarity(job_title, title_list):\n",
    "#     '''\n",
    "#     function take a string and comapre its similarity\n",
    "#     similarity for each job title with this title\n",
    "#     argument:\n",
    "#         user job_title, and all of our job_title\n",
    "#     return:\n",
    "#         sordted list of set with the weights and the index meet this weight in our data \n",
    "#     '''\n",
    "#     weighted_similarity = []\n",
    "#     for indx, title in enumerate(title_list):\n",
    "#         weights = compute_jaccard_similarity_score(title, job_title)\n",
    "#         weighted_similarity.append((weights, indx))\n",
    "#         weighted_similarity.sort(reverse=True)\n",
    "#     return weighted_similarity\n",
    "\n",
    "\n",
    "# def most_related_jobFunction(job_title, df_file_updated):\n",
    "#     '''\n",
    "#         this function return most frequent job function to user\n",
    "#         argument:\n",
    "#             user job_title, our data file\n",
    "#         return:\n",
    "#             most related 5 job function\n",
    "#     '''\n",
    "#     all_related_weighted_similarity = weighted_similarity(job_title, df_file_updated['title'])\n",
    "#     most_related_jobs = []\n",
    "#     for job in range(0, 2):\n",
    "#         most_related_jobs.append(df_file_updated['jobFunction'][all_related_weighted_similarity[job][1]])\n",
    "#     return most_related_jobs\n",
    "\n",
    "\n",
    "# def recommendation_model(file_to_clean, user_job_title):\n",
    "#     '''\n",
    "#         1 - clean the file data\n",
    "#         2 - read cleaned file\n",
    "#         3-  remove null values from cleaned file\n",
    "#         4 - get related job function\n",
    "#         argument:\n",
    "#             file we need to clean for our recommendation\n",
    "#         return:\n",
    "#             related job function\n",
    "        \n",
    "#     '''\n",
    "#     df_file = file_to_clean.drop(['Unnamed: 0', 'industry'],axis=1)\n",
    "#     df_file = file_cleaning(df_file)\n",
    "    \n",
    "#     full_path = os.path.realpath(__file__)\n",
    "#     path, filename = os.path.split(full_path)\n",
    "    \n",
    "#     df_file.to_csv(path + '/jobs_data_updated.csv', index=False)\n",
    "#     df_file_updated = pd.read_csv(path+ '/jobs_data_updated.csv')\n",
    "    \n",
    "#     df_file_updated =  df_file_updated.dropna()\n",
    "    \n",
    "#     user_job_titl = user_job_title\n",
    "#     most_jobfunction = most_related_jobFunction(user_job_title, df_file_updated)\n",
    "\n",
    "#     return most_jobfunction\n",
    "\n",
    "# app = Flask(__name__)\n",
    "# @app.route('/job_function/', methods=['GET'])\n",
    "\n",
    "# def job_function():\n",
    "#     Job_title = request.args['Job_title']\n",
    "#     Job_title = clean_text(Job_title).lower()\n",
    "#     most_jobfunction = recommendation_model(df_file, Job_title)\n",
    "#     job_functions= set()\n",
    "#     for val in most_jobfunction:\n",
    "#         words = remove_stop_words(val)\n",
    "#         for word in words:\n",
    "#             job_functions.add(word.upper())\n",
    "#     job_functions = list(job_functions)\n",
    "#     return jsonify({\"Most Related function for job title of \"+str(Job_title): job_functions})\n",
    "\n",
    "\n",
    "\n",
    "# @app.route('/', methods=['GET'])\n",
    "\n",
    "# def job_title():\n",
    "#     Job_title = request.args['Job_title']\n",
    "#     return redirect(\"http://0.0.0.0:8066/job_function/?Job_title=\"+Job_title, code=302)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     from waitress import serve\n",
    "#     serve(app, host=\"0.0.0.0\", port=8066)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Result of Prediction\n",
    "\n",
    "**First Image Using Modila Firefox browser**\n",
    "    ![alt text](images/marketing_1.png \"Firefox Image\")\n",
    "    \n",
    "**Second Image Using Psot Man Application**\n",
    "    ![alt text](images/marketing_2.png \"Psot Man Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[Creating a RESTFul API With Flask [1 of 4] - Get Requests](https://www.youtube.com/watch?v=CjYKrbq8BCw&feature=youtu.be)\n",
    "\n",
    "[NameError: name 'requests' is not defined - Get Requests](https://stackoverflow.com/questions/26895371/nameerror-name-requests-is-not-defined/26895410)\n",
    "\n",
    "[How can I get the named parameters from a URL using Flask?](https://stackoverflow.com/questions/24892035/how-can-i-get-the-named-parameters-from-a-url-using-flask)\n",
    "\n",
    "[Processing Incoming Request Data in Flask\n",
    "](https://scotch.io/bar-talk/processing-incoming-request-data-in-flask)\n",
    "\n",
    "\n",
    "[8 Ways to Clean Data Using Data Cleaning Techniques](https://www.digitalvidya.com/blog/data-cleaning-techniques/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
